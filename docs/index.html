<!DOCTYPE html>
<html lang="en">

  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@qi2peng2">
    <meta name="twitter:creator" content="@qi2peng2">
    <meta name="twitter:title" content="QuALITY: Question Answering with Long Input Text, Yes!">
    <meta name="twitter:description" content="QuALITY is a multiple-choice question answering dataset with context passages in English that have an average length of about 5,000 tokens.">

    <title>QuALITY Leaderboard</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/clean-blog.min.css" rel="stylesheet">
    <link href="css/quality.css" rel="stylesheet">
    
<script type="application/ld+json">
{
  "@context":"https://schema.org/",
  "@type":"Dataset",
  "name":"QuALITY",
  "description":"Question answering with long input text, yes!",
  "url":"https://nyu-mll.github.io/quality/",
  "keywords":[
     "Natural Language Processing",
     "Question Answering",
  ],
  "creator":{
     "@type":"Organization",
     "url": "https://nyu-mll.github.io/quality/",
     "name":"New York University",
     "contactPoint":{
        "@type":"ContactPoint",
        "contactType": "customer service",
        "url":"https://yzpang.me/",
        "email":"yzpang@nyu.edu"
     }
  },
}
</script>
    

  </head>

  <body>
<!-- <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="index.html"></a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      Menu
      <i class="fa fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
      </ul>
    </div>
  </div>
</nav> -->
<!-- Page Header -->
<header class="masthead" style="background-color:tomato;">
<!-- <header class="masthead" style="background-image: url('img/home-bg.jpeg')">
 -->  <div class="overlay"></div>
  <div class="topleft"> <img id="mypic" src="img/read-white.png" alt="" class="img-responsive" object-fit="scale-down" style="position:absolute; left:5%; top:10%;" /> </div>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <div class="site-heading">
          <h1>QuALITY</h1>
          <span class="subheading">Question Answering with Long Input Texts, Yes!</span></span>
        </div>
      </div>
    </div>
  </div>
</header>









<!-- Main Content -->
<div class="container" id="main">
  <div class="row">
    <div class="col-lg-3">
      <div class="list-group">
        <div class="list-group-item">
        <p>QuALITY is a multiple-choice question answering dataset with context passages in English that have an average length of about 5,000 tokens. QuALITY is distributed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/" style="text-decoration: none;">CC BY 4.0 License</a>. The dataset can be downloaded from the repo <a href="https://github.com/nyu-mll/quality" style="text-decoration: none;">here</a>. For more details about QuALITY, please refer to the paper: <a href="https://arxiv.org/pdf/2112.08608.pdf" style="text-decoration: none;">Pang et al. (2022)</a>.</p>
        <p class="text-center">
          
        </p>
        </div>
        <div class="list-group-item">
          <!-- <h4>Submission</h4> -->

        For submission instructions, please refer to <a href="https://github.com/nyu-mll/quality/blob/main/leaderboard/submission.md" style="text-decoration: none;">this page</a>.</p>
        </div>
        <div class="list-group-item">
        <pre>
@inproceedings{pang-etal-2022-quality,
    title = "{Q}u{ALITY}: Question Answering with Long Input Texts, Yes!",
    author = "Pang, Richard Yuanzhe  and
      Parrish, Alicia  and
      Joshi, Nitish  and
      Nangia, Nikita  and
      Phang, Jason  and
      Chen, Angelica  and
      Padmakumar, Vishakh  and
      Ma, Johnny  and
      Thompson, Jana  and
      He, He  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.391",
    pages = "5336--5358",
    abstract = "To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Our baseline models perform poorly on this task (55.4{\%}) and significantly lag behind human performance (93.5{\%}).",
}
}</pre>
    </div>
  </div>
</div>
<div class="col-lg-9">
  <div class="card card-outline-secondary">
    <div class="card-header">
      Leaderboard (last updated: January 2025)
    </div>
    <div class="card-body">
      Important notes:
      <ul>
      <li> Rankings are determined by the accuracy on the entire test set. </li>
      <li> Accuracy = (number of correct answers) / (num of examples). 
      <li> SAT-style score = (number of correct answers - (1/3) * number of incorrect answers + 0 * number of abstained answers) / (number of examples). </li>
      </ul>

      Updates:
      <ul>
      <li> [2023/5] Please also refer to the <a href="https://www.scrolls-benchmark.com/leaderboard" style="text-decoration: none;">SCROLLS benchmark</a> which includes the QuALITY task; as of May 2023, the top QuALITY accruacy on SCROLLS is 48.1 (test set) / 43.8 (hard subset of the test set) by CoLT5 XL. </li>
      <li> [2022/11] We have added promising but unranked results at the bottom of the table. </li>
      </ul>
      <table class="table table-responsive">
        <thead class="thead-light">
          <tr>
            <th scope="col" rowspan=2></th>
            <th scope="col" rowspan=2 class="align-middle text-center">Model name</th>
            <th scope="col" rowspan=2 class="align-middle text-center"></th>
            <th scope="col" rowspan=2 class="align-middle text-center">Paper</th>
            <th scope="col" rowspan=2 class="align-middle text-center">Code</th>
            <th scope="col" colspan=2 class='text-center'>Accuracy</th>
            <th scope="col" colspan=2 class='text-center'>SAT-style score</th>
          </tr>
          <tr>
            <th scope="col" class='text-center'>Test set</th>
            <th scope="col" class='text-center'>Hard subset</th>
            <th scope="col" class='text-center'>Test set</th>
            <th scope="col" class='text-center'>Hard subset</th>
          </tr>
        </thead>




<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>0<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Human annotators<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000008" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'>–</td>
    <td class='align-middle text-center'><strong>93.5</strong></td>
    <td class='align-middle text-center'><strong>89.1</strong></td>
    <td class='align-middle text-center'><strong>91.4</strong></td>
    <td class='align-middle text-center'><strong>85.4</strong></td>
</tr>
<tr id="000008" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: We estimate human accuracy on QuALITY on a random sample of 20 passages (367 questions). Each question is annotated by 3 new validation annotators who had not previously annotated that passage, and whose labels do not contribute to the assignment of the gold label. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>1<br>
      <span class="badge badge-secondary">2024/09</span></td>
    <td class='align-middle text-left'>Baseline model: RAPTOR + gpt-4o w/ query intent & entity understanding<br/>
      <span class="affiliation">powerdrill.ai</span><br/></td>
    <td class='align-middle text-center'><button data-id="000016" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'><strong>83.1</strong></td>
    <td class='align-middle text-center'><strong>77.3</strong></td>
    <td class='align-middle text-center'><strong>77.5</strong></td>
    <td class='align-middle text-center'><strong>69.7</strong></td>
</tr>
<tr id="000016" style="display:none;" class="section" >
<td colspan="8" style="font-size:13px"> Model description: Based on RAPTOR, we first decompose user query into a multi-hop plan. For each plan, key  entities are extracted to precisely match corresponding information. We also use a rerank model to filter out unnecessary chunk pieces to avoid long context for cost saving and latency improvement. [dev@powerdrill.ai]</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>2<br>
      <span class="badge badge-secondary">2023/06</span></td>
    <td class='align-middle text-left'>RAPTOR (collapsed tree) + GPT-4<br/>
      <span class="affiliation">Stanford University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000012" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://openreview.net/attachment?id=GN921JHCRw&name=pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'><strong>82.6</strong></td>
    <td class='align-middle text-center'><strong>76.2</strong></td>
    <td class='align-middle text-center'><strong>77.5</strong></td>
    <td class='align-middle text-center'><strong>69.3</strong></td>
</tr>
<tr id="000012" style="display:none;" class="section" >
<td colspan="8" style="font-size:13px"> Model description: RAPTOR recursively clusters chunks of text and generates text summaries of those clusters constructing a summarization tree from the bottom-up. At inference time, RAPTOR retrieves from this tree, allowing it to integrate information across large text corpora at varying levels of abstraction. RAPTOR employs a novel variant of the Gaussian Mixture Model (GMM) for text clustering and clusters the text after dimensionality reduction on the embeddings using Uniform Manifold Approximation and Projection (UMAP). Then, the formed clusters are summarized using a large language model (LLM). The generated summary text is again subjected to clustering. This process is iteratively performed for a predetermined number of layers. The outcome of this iterative process is a bottom-up hierarchical tree structure, wherein each node signifies a cluster of related text chunks. RAPTOR uses SBERT embeddings for clustering.</td>
</tr></tbody>


<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>3<br>
      <span class="badge badge-secondary">2024/01</span></td>
    <td class='align-middle text-left'>Baseline model: Long-context GPT-3.5 (gpt-3.5-turbo-16k) as of January 2024<br/>
      <span class="affiliation">Anonymous</span><br/></td>
    <td class='align-middle text-center'><button data-id="000014" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'><strong>74.7</strong></td>
    <td class='align-middle text-center'><strong>64.3</strong></td>
    <td class='align-middle text-center'><strong>66.2</strong></td>
    <td class='align-middle text-center'><strong>52.4</strong></td>
</tr>
<tr id="000014" style="display:none;" class="section" >
<td colspan="8" style="font-size:13px"> Model description: This entry is submitted anonymously. The numbers reflect the zero-shot performance of gpt-3.5-turbo-16k as of January 2024, but the specific prompt used is unclear. </td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>4<br>
      <span class="badge badge-secondary">2023/10</span></td>
    <td class='align-middle text-left'>LongMA: Fine-Tuning TechGPT-7B using QLoRA on QuALITY and RACE subset<br/>
      <span class="affiliation">Qi Ma, Northeastern University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000013" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'><strong>73.0</strong></td>
    <td class='align-middle text-center'><strong>64.0</strong></td>
    <td class='align-middle text-center'><strong>64.8</strong></td>
    <td class='align-middle text-center'><strong>53.0</strong></td>
</tr>
<tr id="000013" style="display:none;" class="section" >
<td colspan="8" style="font-size:13px"> Model description: We utilize TechGPT to identify the most pertinent contexts for a given question and set of options, which is used to create subsets derived from the QuALITY and RACE datasets. To ensure compatibility, we limit the maximum token length to 2048 for the question, options, and contexts. During the training phase, we use QLoRA to fine-tune the TechGPT-7b model. In the first stage, we fine-tuned 2 epochs on the RACE subset, followed by a second stage of fine-tuning on the QuALITY subset for 2 epochs.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>5<br>
      <span class="badge badge-secondary">2024/12</span></td>
    <td class='align-middle text-left'>RAPTOR+GPT-4o-mini, initial cluster centers determined through subqueries<br/>
      <span class="affiliation">Guanran Luo, Xiamen University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000017" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'><strong>71.7</strong></td>
    <td class='align-middle text-center'><strong>60.0</strong></td>
    <td class='align-middle text-center'><strong>62.3</strong></td>
    <td class='align-middle text-center'><strong>46.7</strong></td>
</tr>
<tr id="000017" style="display:none;" class="section" >
<td colspan="8" style="font-size:13px"> Model description: Based on RAPTOR, we first decompose the query into multiple subqueries by incorporating the document structure, using these subqueries as the initial cluster centers for the first layer of recursive summarization. Additionally, we have eliminated global clustering. This approach helps improve the efficiency of document tree generation and search, and makes the generated summaries and queries more relevant. [30920240157704 at stu.xmu.edu.cn]</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>6<br>
      <span class="badge badge-secondary">2022/05</span></td>
    <td class='align-middle text-left'>CoLISA: DPR & DeBERTaV3-large architecture plus contrastive learning & in-sample attention<br/>
      <span class="affiliation">SUDA NLP & I2R at Soochow University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000010" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://link.springer.com/chapter/10.1007/978-3-031-28244-7_17"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/Walle1493/CoLISA"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>62.3</strong></td>
    <td class='align-middle text-center'><strong>54.7</strong></td>
    <td class='align-middle text-center'><strong>49.7</strong></td>
    <td class='align-middle text-center'><strong>39.6</strong></td>
</tr>
<tr id="000010" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: We extract a short context from the original passage by DPR (using questions and options). Then we successively fine-tune a multiple-choice model on RACE and QuALITY. Besides, we introduce contrastive learning method and in-sample attention mechanism within each sample to better distinguish the answers from the distractors. External data/resources: We use the off-the-shelf DPR retriever to extract the short contexts from original articles. And we use the pre-trained DeBERTaV3-large model on Hugging Face. Besides, RACE is introduced into our first stage of fine tuning the model.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>7<br>
      <span class="badge badge-secondary">2022/04</span></td>
    <td class='align-middle text-left'>CoLISA: DPR & DeBERTaV3-large architecture & contrastive learning<br/>
      <span class="affiliation">SUDA NLP & I2R at Soochow University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000009" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://link.springer.com/chapter/10.1007/978-3-031-28244-7_17"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/Walle1493/CoLISA"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>62.1</strong></td>
    <td class='align-middle text-center'><strong>54.3</strong></td>
    <td class='align-middle text-center'><strong>49.5</strong></td>
    <td class='align-middle text-center'><strong>39.1</strong></td>
</tr>
<tr id="000009" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: We extract a short context from the original passage by DPR (using questions and options). Then we successively fine-tune a multiple-choice model on RACE and QuALITY. Besides, we introduce contrastive learning method within each sample to better distinguish the answers from the distractors. External data/resources: We use the off-the-shelf DPR retriever to extract the short contexts from original articles. And we use the pre-trained DeBERTaV3-large model on Hugging Face. Besides, RACE is introduced into our first stage of fine tuning the model.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>8<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: DPR retrieval using questions & DeBERTaV3-large with intermediate training on RACE<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000005" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>55.4</strong></td>
    <td class='align-middle text-center'><strong>46.1</strong></td>
    <td class='align-middle text-center'><strong>40.5</strong></td>
    <td class='align-middle text-center'><strong>28.1</strong></td>
</tr>
<tr id="000005" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: We use DeBERTaV3-large and first do intermediate training on RACE; then we fine-tune the model on QuALITY. For fine-tuning, we use the similarity (based on DPR) between each source sentence and the question to select shorter contexts to feed into the QA model. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>9<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: DPR retrieval using questions & RoBERTa-large with intermediate training on RACE<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000003" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>51.4</strong></td>
    <td class='align-middle text-center'><strong>44.7</strong></td>
    <td class='align-middle text-center'><strong>35.2</strong></td>
    <td class='align-middle text-center'><strong>26.3</strong></td>
</tr>
<tr id="000003" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: We use RoBERTa-large and first do intermediate training on RACE; then we fine-tune the model on QuALITY. For fine-tuning, we use the similarity (based on DPR) between each source sentence and the question to select shorter contexts to feed into the QA model. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>10<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: DPR retrieval using questions & DeBERTaV3-large <br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000004" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>49.0</strong></td>
    <td class='align-middle text-center'><strong>41.2</strong></td>
    <td class='align-middle text-center'><strong>32.0</strong></td>
    <td class='align-middle text-center'><strong>21.6</strong></td>
</tr>
<tr id="000004" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: We fine-tune DeBERTa-V3-large on QuALITY. We use the similarity (based on DPR) between each source sentence and the question to select shorter contexts to feed into the QA model. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>11<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Question-only baseline: DeBERTaV3-large with intermediate training on RACE<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000007" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>43.3</strong></td>
    <td class='align-middle text-center'><strong>38.2</strong></td>
    <td class='align-middle text-center'><strong>24.4</strong></td>
    <td class='align-middle text-center'><strong>17.6</strong></td>
</tr>
<tr id="000007" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: We use DeBERTaV3-large and first do intermediate training on RACE; then we fine-tune the model on QuALITY. For fine-tuning the input only consists of the questions and options (so, no articles).</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>12<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: fastText retrieval using questions & RoBERTa-large<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000002" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>42.7</strong></td>
    <td class='align-middle text-center'><strong>35.7</strong></td>
    <td class='align-middle text-center'><strong>23.6</strong></td>
    <td class='align-middle text-center'><strong>14.3</strong></td>
</tr>
<tr id="000002" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: We fine-tune RoBERTa-large on QuALITY. We use the similarity (based on fastText) between each source sentence and the question to select shorter contexts to feed into the QA model. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>13<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Question-only baseline: DeBERTaV3-large<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000006" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>39.7</strong></td>
    <td class='align-middle text-center'><strong>35.2</strong></td>
    <td class='align-middle text-center'><strong>19.6</strong></td>
    <td class='align-middle text-center'><strong>13.5</strong></td>
</tr>
<tr id="000006" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: Model description: We fine-tune DeBERTa-V3-large on QuALITY but the input only consists of the questions and options (so, no articles).</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>14<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: Longformer with intermediate training on RACE<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000001" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>39.5</strong></td>
    <td class='align-middle text-center'><strong>35.3</strong></td>
    <td class='align-middle text-center'><strong>19.4</strong></td>
    <td class='align-middle text-center'><strong>13.8</strong></td>
</tr>
<tr id="000001" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: We first do intermediate training using a pretrained Longformer (which supports up to 4,096 tokens) on RACE. Then, we fine-tune the model on QuALITY. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>15<br>
      <span class="badge badge-secondary">2024/01</span></td>
    <td class='align-middle text-left'>Baseline model: Vicuna-7B<br/>
      <span class="affiliation">Anonymous</span><br/></td>
    <td class='align-middle text-center'><button data-id="000015" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'><strong>39.1</strong></td>
    <td class='align-middle text-center'><strong>33.9</strong></td>
    <td class='align-middle text-center'><strong>18.8</strong></td>
    <td class='align-middle text-center'><strong>11.9</strong></td>
</tr>
<tr id="000015" style="display:none;" class="section" >
<td colspan="8" style="font-size:13px"> Model description: This entry is submitted anonymously. The numbers reflect the zero-shot performance of Vicuna-7B as of January 2024, but the specific prompt used is unclear. </td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>16<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: Longformer<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000000" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>30.7</strong></td>
    <td class='align-middle text-center'><strong>29.3</strong></td>
    <td class='align-middle text-center'><strong>7.6</strong></td>
    <td class='align-middle text-center'><strong>5.7</strong></td>
</tr>
<tr id="000000" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: We fine-tune a pretrained Longformer which supports up to 4,096 tokens, on QuALITY. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>--<br>
      <span class="badge badge-secondary">2022/11</span></td>
    <td class='align-middle text-left'>Best-of-20 chain-of-thought w/ a 52B-parameter LM (Bai et al., 2022) fine-tuned by reinforcement learning with human feedback (RLHF) [Note: added by QuALITY authors; unranked given that performance is on dev set only]<br/>
      <span class="affiliation">Anthropic, Surge AI</span><br/></td>
    <td class='align-middle text-center'><button data-id="000011" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2204.05862.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a><br>&<br><a href="https://arxiv.org/pdf/2211.03540.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/anthropics/hh-rlhf"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>66.9</strong></td>
    <td class='align-middle text-center'><strong>–</strong></td>
    <td class='align-middle text-center'><strong>–</strong></td>
    <td class='align-middle text-center'><strong>–</strong></td>
</tr>
<tr id="000011" style="display:none;" class="section" >
<td colspan="9" style="font-size:13px"> Model description: This submission is manually added by QuALITY authors. From the paper: "We prompt the model with a chain-of-thought-style input before asking them to answer (Nye et al., 2021; Wei et al., 2022). We sample 20 instances of model reasoning and choose the one that scores best under our RLHF preference model before conditioning on that reasoning string to generate the answer."</td>
</tr></tbody>



          <script>
              $('.display-description').on("click", function () {
                $(this).parents('tr').next().toggle();
              });
          </script>


      </table>
    </div>
  </div>
  </div>
</div>
</div>



    <hr>
    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <p class="copyright text-muted">Copyright &copy; QuALITY Team, 2022.</p>
            <p class="copyright text-muted">The homepage is adapted from <a href="https://beerqa.github.io/" style="text-decoration: none;">BeerQA</a> and <a href="https://hotpotqa.github.io/" style="text-decoration: none;">HotpotQA</a> (with permission) which are in turn adapted from Start Bootstrap's <a href="https://startbootstrap.com/template-overviews/clean-blog/" style="text-decoration: none;">Clean Blog</a> template.</p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script>$(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })</script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>
    

  </body>

</html>
