<!DOCTYPE html>
<html lang="en">

  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@qi2peng2">
    <meta name="twitter:creator" content="@qi2peng2">
    <meta name="twitter:title" content="QuALITY: Question Answering with Long Input Text, Yes!">
    <meta name="twitter:description" content="QuALITY is a multiple-choice question answering dataset with context passages in English that have an average length of about 5,000 tokens.">

    <title>QuALITY Leaderboard</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/clean-blog.min.css" rel="stylesheet">
    <link href="css/quality.css" rel="stylesheet">
    
<script type="application/ld+json">
{
  "@context":"https://schema.org/",
  "@type":"Dataset",
  "name":"QuALITY",
  "description":"Question answering with long input text, yes!",
  "url":"https://nyu-mll.github.io/quality/",
  "keywords":[
     "Natural Language Processing",
     "Question Answering",
  ],
  "creator":{
     "@type":"Organization",
     "url": "https://nyu-mll.github.io/quality/",
     "name":"New York University",
     "contactPoint":{
        "@type":"ContactPoint",
        "contactType": "customer service",
        "url":"https://yzpang.me/",
        "email":"yzpang@nyu.edu"
     }
  },
}
</script>
    

  </head>

  <body>
<!-- <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="index.html"></a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      Menu
      <i class="fa fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
      </ul>
    </div>
  </div>
</nav> -->
<!-- Page Header -->
<header class="masthead" style="background-color:tomato;">
<!-- <header class="masthead" style="background-image: url('img/home-bg.jpeg')">
 -->  <div class="overlay"></div>
  <div class="topleft"> <img id="mypic" src="img/read-white.png" alt="" class="img-responsive" object-fit="scale-down" style="position:absolute; left:5%; top:10%;" /> </div>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <div class="site-heading">
          <h1>QuALITY</h1>
          <span class="subheading">Question Answering with Long Input Texts, Yes!</span></span>
        </div>
      </div>
    </div>
  </div>
</header>









<!-- Main Content -->
<div class="container" id="main">
  <div class="row">
    <div class="col-lg-3">
      <div class="list-group">
        <div class="list-group-item">
        <p>QuALITY is a multiple-choice question answering dataset with context passages in English that have an average length of about 5,000 tokens. QuALITY is distributed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/" style="text-decoration: none;">CC BY 4.0 License</a>. The dataset can be downloaded from the repo <a href="https://github.com/nyu-mll/quality" style="text-decoration: none;">here</a>. For more details about QuALITY, please refer to the paper: <a href="https://arxiv.org/pdf/2112.08608.pdf" style="text-decoration: none;">Pang et al. (2022)</a>.</p>
        <p class="text-center">
          
        </p>
        </div>
        <div class="list-group-item">
          <!-- <h4>Submission</h4> -->

        For submission instructions, please refer to <a href="https://github.com/nyu-mll/quality/blob/main/leaderboard/submission.md" style="text-decoration: none;">this page</a>.</p>
        </div>
        <div class="list-group-item">
        <pre>
@inproceedings{pang-etal-2022-quality,
    title = "{Q}u{ALITY}: Question Answering with Long Input Texts, Yes!",
    author = "Pang, Richard Yuanzhe  and
      Parrish, Alicia  and
      Joshi, Nitish  and
      Nangia, Nikita  and
      Phang, Jason  and
      Chen, Angelica  and
      Padmakumar, Vishakh  and
      Ma, Johnny  and
      Thompson, Jana  and
      He, He  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.391",
    pages = "5336--5358",
    abstract = "To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Our baseline models perform poorly on this task (55.4{\%}) and significantly lag behind human performance (93.5{\%}).",
}
}</pre>
    </div>
  </div>
</div>
<div class="col-lg-9">
  <div class="card card-outline-secondary">
    <div class="card-header">
      Leaderboard
    </div>
    <div class="card-body">
      <ul>
      <li> Rankings are determined by the accuracy on the entire test set. </li>
      <li> Accuracy = (number of correct answers) / (num of examples). 
      <li> SAT-style score = (number of correct answers - (1/3) * number of incorrect answers + 0 * number of abstained answers) / (number of examples). </li>
      </ul>

      Updates:
      <ul>
      <li> [2022/11] We have added promising but unranked results at the bottom of the table. Top dev-set performance is currently 66.9.</li>
      <li> [2022/12] Please also refer to the <a href="https://www.scrolls-benchmark.com/leaderboard" style="text-decoration: none;">SCROLLS benchmark</a> which includes the QuALITY task; as of November 2022, the top QuALITY accruacy on SCROLLS is 46.0 (test set) / 42.1 (hard subset) by LongT5 XL. </li>
      </ul>
      <table class="table table-responsive">
        <thead class="thead-light">
          <tr>
            <th scope="col" rowspan=2></th>
            <th scope="col" rowspan=2 class="align-middle text-center">Model name</th>
            <th scope="col" rowspan=2 class="align-middle text-center"></th>
            <th scope="col" rowspan=2 class="align-middle text-center">Paper</th>
            <th scope="col" rowspan=2 class="align-middle text-center">Code</th>
            <th scope="col" colspan=2 class='text-center'>Accuracy</th>
            <th scope="col" colspan=2 class='text-center'>SAT-style score</th>
          </tr>
          <tr>
            <th scope="col" class='text-center'>Test set</th>
            <th scope="col" class='text-center'>Hard subset</th>
            <th scope="col" class='text-center'>Test set</th>
            <th scope="col" class='text-center'>Hard subset</th>
          </tr>
        </thead>




<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>0<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Human annotators<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000008" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'><strong>93.5</strong></td>
    <td class='align-middle text-center'><strong>89.1</strong></td>
    <td class='align-middle text-center'><strong>91.4</strong></td>
    <td class='align-middle text-center'><strong>85.4</strong></td>
</tr>
<tr id="000008" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We estimate human accuracy on QuALITY on a random sample of 20 passages (367 questions). Each question is annotated by 3 new validation annotators who had not previously annotated that passage, and whose labels do not contribute to the assignment of the gold label. See paper for details.</td>
</tr></tbody>


<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>1<br>
      <span class="badge badge-secondary">2022/05</span></td>
    <td class='align-middle text-left'>CoLISA: DPR & DeBERTaV3-large architecture plus contrastive learning & in-sample attention<br/>
      <span class="affiliation">Mengxing Dong</span><br/></td>
    <td class='align-middle text-center'><button data-id="000010" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'><strong>62.3</strong></td>
    <td class='align-middle text-center'><strong>54.7</strong></td>
    <td class='align-middle text-center'><strong>49.7</strong></td>
    <td class='align-middle text-center'><strong>39.6</strong></td>
</tr>
<tr id="000010" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We extract a short context from the original passage based on DPR model. Then we successively fine-tune a multiple-choice model on RACE and QuALITY. Besides, we introduce contrastive learning method and in-sample attention mechanism within each sample to better distinguish the answers from the distractors. External data/resources: We use the off-the-shelf DPR retriever to extract the short contexts from original articles. And we use the pre-trained DeBERTaV3-large model on Hugging Face. Besides, RACE is introduced into our first stage of fine tuning the model.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>2<br>
      <span class="badge badge-secondary">2022/04</span></td>
    <td class='align-middle text-left'>CoLISA: DPR & DeBERTaV3-large architecture & contrastive learning<br/>
      <span class="affiliation">Mengxing Dong</span><br/></td>
    <td class='align-middle text-center'><button data-id="000009" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'></td>
    <td class='align-middle text-center'><strong>62.1</strong></td>
    <td class='align-middle text-center'><strong>54.3</strong></td>
    <td class='align-middle text-center'><strong>49.5</strong></td>
    <td class='align-middle text-center'><strong>39.1</strong></td>
</tr>
<tr id="000009" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We extract a short context from the original passage based on DPR model. Then we successively fine-tune a multiple-choice model on RACE and QuALITY. Besides, we introduce contrastive learning method within each sample to better distinguish the answers from the distractors. External data/resources: We use the off-the-shelf DPR retriever to extract the short contexts from original articles. And we use the pre-trained DeBERTaV3-large model on Hugging Face. Besides, RACE is introduced into our first stage of fine tuning the model.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>3<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: DPR retrieval using questions & DeBERTaV3-large with intermediate training on RACE<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000005" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>55.4</strong></td>
    <td class='align-middle text-center'><strong>46.1</strong></td>
    <td class='align-middle text-center'><strong>40.5</strong></td>
    <td class='align-middle text-center'><strong>28.1</strong></td>
</tr>
<tr id="000005" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We use DeBERTaV3-large and first do intermediate training on RACE; then we fine-tune the model on QuALITY. For fine-tuning, we use the similarity (based on DPR) between each source sentence and the question to select shorter contexts to feed into the QA model. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>4<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: DPR retrieval using questions & RoBERTa-large with intermediate training on RACE<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000003" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>51.4</strong></td>
    <td class='align-middle text-center'><strong>44.7</strong></td>
    <td class='align-middle text-center'><strong>35.2</strong></td>
    <td class='align-middle text-center'><strong>26.3</strong></td>
</tr>
<tr id="000003" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We use RoBERTa-large and first do intermediate training on RACE; then we fine-tune the model on QuALITY. For fine-tuning, we use the similarity (based on DPR) between each source sentence and the question to select shorter contexts to feed into the QA model. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>5<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: DPR retrieval using questions & DeBERTaV3-large <br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000004" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>49.0</strong></td>
    <td class='align-middle text-center'><strong>41.2</strong></td>
    <td class='align-middle text-center'><strong>32.0</strong></td>
    <td class='align-middle text-center'><strong>21.6</strong></td>
</tr>
<tr id="000004" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We fine-tune DeBERTa-V3-large on QuALITY. We use the similarity (based on DPR) between each source sentence and the question to select shorter contexts to feed into the QA model. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>6<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Question-only baseline: DeBERTaV3-large with intermediate training on RACE<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000007" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>43.3</strong></td>
    <td class='align-middle text-center'><strong>38.2</strong></td>
    <td class='align-middle text-center'><strong>24.4</strong></td>
    <td class='align-middle text-center'><strong>17.6</strong></td>
</tr>
<tr id="000007" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We use DeBERTaV3-large and first do intermediate training on RACE; then we fine-tune the model on QuALITY. For fine-tuning the input only consists of the questions and options (so, no articles).</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>7<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: fastText retrieval using questions & RoBERTa-large<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000002" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>42.7</strong></td>
    <td class='align-middle text-center'><strong>35.7</strong></td>
    <td class='align-middle text-center'><strong>23.6</strong></td>
    <td class='align-middle text-center'><strong>14.3</strong></td>
</tr>
<tr id="000002" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We fine-tune RoBERTa-large on QuALITY. We use the similarity (based on fastText) between each source sentence and the question to select shorter contexts to feed into the QA model. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>8<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Question-only baseline: DeBERTaV3-large<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000006" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>39.7</strong></td>
    <td class='align-middle text-center'><strong>35.2</strong></td>
    <td class='align-middle text-center'><strong>19.6</strong></td>
    <td class='align-middle text-center'><strong>13.5</strong></td>
</tr>
<tr id="000006" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We fine-tune DeBERTa-V3-large on QuALITY but the input only consists of the questions and options (so, no articles).</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>9<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: Longformer with intermediate training on RACE<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000001" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>39.5</strong></td>
    <td class='align-middle text-center'><strong>35.3</strong></td>
    <td class='align-middle text-center'><strong>19.4</strong></td>
    <td class='align-middle text-center'><strong>13.8</strong></td>
</tr>
<tr id="000001" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We first do intermediate training using a pretrained Longformer (which supports up to 4,096 tokens) on RACE. Then, we fine-tune the model on QuALITY. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>10<br>
      <span class="badge badge-secondary">2021/12</span></td>
    <td class='align-middle text-left'>Baseline model: Longformer<br/>
      <span class="affiliation">New York University</span><br/></td>
    <td class='align-middle text-center'><button data-id="000000" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2112.08608.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/nyu-mll/quality"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>30.7</strong></td>
    <td class='align-middle text-center'><strong>29.3</strong></td>
    <td class='align-middle text-center'><strong>7.6</strong></td>
    <td class='align-middle text-center'><strong>5.7</strong></td>
</tr>
<tr id="000000" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: We fine-tune a pretrained Longformer which supports up to 4,096 tokens, on QuALITY. See paper for details.</td>
</tr></tbody>

<tbody><tr style="background:#f4f4f4">
    <td scope="row" class='align-middle text-center'>--<br>
      <span class="badge badge-secondary">2022/11</span></td>
    <td class='align-middle text-left'>Best-of-20 chain-of-thought, using a 52B-parameter LM (Bai et al., 2022) fine-tuned by reinforcement learning with human feedback (RLHF) [Note: added by QuALITY authors; unranked given that performance is on dev set only]<br/>
      <span class="affiliation">Anthropic, Surge AI</span><br/></td>
    <td class='align-middle text-center'><button data-id="000011" type="submit" class="display-description";>description</button></td>
    <td class='align-middle text-center'><a href="https://arxiv.org/pdf/2204.05862.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a><br>&<br><a href="https://arxiv.org/pdf/2211.03540.pdf"><i class="far fa-file-alt" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><a href="https://github.com/anthropics/hh-rlhf"><i class="fas fa-code" style="color:#32CD32"></i></a></td>
    <td class='align-middle text-center'><strong>66.9</strong></td>
    <td class='align-middle text-center'><strong>--</strong></td>
    <td class='align-middle text-center'><strong>--</strong></td>
    <td class='align-middle text-center'><strong>--</strong></td>
</tr>
<tr id="000011" style="display:none;" class="section" >
<td colspan="7" style="font-size:13px"> Model description: This submission is manually added by QuALITY authors. From the paper: "We prompt the model with a chain-of-thought-style input before asking them to answer (Nye et al., 2021; Wei et al., 2022). We sample 20 instances of model reasoning and choose the one that scores best under our RLHF preference model before conditioning on that reasoning string to generate the answer."</td>
</tr></tbody>



          <script>
              $('.display-description').on("click", function () {
                $(this).parents('tr').next().toggle();
              });
          </script>


      </table>
    </div>
  </div>
  </div>
</div>
</div>



    <hr>
    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <p class="copyright text-muted">Copyright &copy; QuALITY Team, 2022.</p>
            <p class="copyright text-muted">The homepage is adapted from <a href="https://beerqa.github.io/" style="text-decoration: none;">BeerQA</a> and <a href="https://hotpotqa.github.io/" style="text-decoration: none;">HotpotQA</a> (with permission) which are in turn adapted from Start Bootstrap's <a href="https://startbootstrap.com/template-overviews/clean-blog/" style="text-decoration: none;">Clean Blog</a> template.</p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script>$(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })</script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>
    

  </body>

</html>
